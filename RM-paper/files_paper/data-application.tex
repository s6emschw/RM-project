\section{Data application}
\label{section:data}

For the last section of the paper, we seek to analyze the statistical properties of regularization methods using a real data set. We use the prostate cancer \href{https://hastie.su.domains/ElemStatLearn/datasets/prostate.data}{data set} originally used by \cite{stamey1989prostate} and later implemented by \cite{zou2005regularization}. For the sake of comparability, we employ the same train-test split used by \cite{zou2005regularization}. The data set contains information that allows us to explore the relationship between log prostate specific antigen (lpsa) levels, which is
elevated in men with prostate cancer, and 8 clinical measures: log cancer volume (lcavol), log prostate weight (lweight), age, log amount of benign prostatic hyperplasia (lbph), seminal vesicle invasion (svi), log capsular penetration (lcp), Gleason score (gleason), and the percentage Gleason score of 4
or 5 (pgg45). Figure \ref{fig:descr_stats} displays descriptive statistics of the data set. \\

\noindent The data set includes $97$ observations in total, $67$ of which are assigned to the training set and $30$ to the test set. As depicted in Figure \ref{fig:cor_matr}, the data exhibits medium-to-high correlation among regressors (the highest being a correlation factor of $0.75$ between pgg45 and gleason). Hence, this data set seems to be in line with case \ref{tab:case2} (low dimensionality, low sparsity, moderate-high correlation) of our simulation study. We fit each regularization method in the training set and tune the $\lambda$ parameter using 10-fold cross-validation. We then compare their performance by computing the prediction MSE on the test set. \\

\noindent Figure \ref{fig:data_mse_path} depicts the test MSE paths along the $\lambda$ shrinkage penalty for all three regularization models, and it reflects what we expect from the simulation section: For a low range of $\lambda$, there is no difference between the regularization methods and the baseline OLS. We then observe an interval in which the regularization methods outperform OLS. Eventually, as $\lambda$ continues to increase, the regularization methods yield a higher test MSE, indicating a sub-optimal model fit. Additionally, Figure \ref{fig:data_coef_path} in the Appendix shows the coefficient paths of the regularization methods as a function of $\lambda$. \\

\noindent Table \ref{tab:coef_table} contains the coefficient estimates of each model and Table \ref{tab:mod_sel} summarizes their performance by comparing the test MSE. Robust standard errors of the regularization methods are not reported in Table \ref{tab:coef_table}, as their computation is not straightforward and requires complex techniques that are beyond the scope of our research.\footnote{Interested readers can refer to \cite{vinod1995double}, \cite{chatterjee2011bootstrapping}, and \cite{casella2010penalized}.} Concerning model selection, OLS performs the worst. As reported by \cite{zou2005regularization}, the results of the naive elastic net regression are identical to those for the ridge regression and thus fail to perform variable selection. In this regard, though the tuned $\lambda$ differs for ridge and the naive elastic net, they perform equivalently in terms of test MSE due to the flattening of the MSE curve after some $\lambda$ threshold.\footnote{See Section \ref{section:software} in the Appendix for further details.} According to Figure \ref{fig:data_coef_path}, the lasso model fails to perform variable selection, as the $\lambda$ selected by cross-validation estimates all coefficients to be non-zero. To further explore this issue, we find that lasso performs best if the LARS algorithm is implemented and the Akaike information criterion (AIC) is used for tuning the $\lambda$ parameter.\footnote{Figure \ref{fig:AIC} in the Appendix visually shows the optimum $\lambda$ that minimizes the AIC.} As such, it selects the same number of variables as indicated by the authors. A detailed explanation about AIC can be found in the Appendix.





