\section{Simulation studies}
\label{section:simulation}

\noindent In the following section, we present the main results of Monte-Carlo simulations to demonstrate the effects of a traditional OLS regression in the presence of high dimensionality and/or multicollinearity. We then proceed with a number of further simulation exercises that depict the basic characteristics of ridge, lasso, and elastic-net regressions already described in Section \ref{section:theory} and subsequently show how these regularization methods mitigate the potential drawbacks of the OLS model. \\

\noindent As a general note for this section, we compute the lasso regression using the coordinate descent algorithm, as it has been shown to be computationally faster than the LARS algorithm. However, in Section \ref{section:data} we include a lasso estimation through LARS as well to replicate the results from the real data analysis outlined in \cite{zou2005regularization}. Furthermore, we were only able to compute the naive elastic net regression, since the current Python scikit-learn library does not support the estimation of a robust elastic net regression. Details on this can be found in Section \ref{section:software}.

\subsection{The data generating process}

For the simulation exercises discussed below, we use the following data generating process (DGP): we draw i.i.d random variables from a Gaussian distribution $X \sim \mathcal{N}_{n}(\mu,\,\sigma^{2})\,$, with $\mu=0$ and $\sigma^{2}=1$. Assuming $\varepsilon \sim \mathcal{N}_{n}(0, 1)$, the true model is given by: 
\begin{align}
\label{eqn:truemodel}
\mathbf{y}= \mathbf{X}\beta + \varepsilon.
\end{align}

\noindent We continuously vary the vector $\beta$ such that it takes both zero and non-zero values. We later modify the DGP by introducing some degree of pairwise correlation between the features, where we vary the amount of multicollinearity using the following formulation for the correlation matrix:  
\begin{align}
\label{eqn:corrmatrix}
corr(\textit{i,j}) = \text{correlation factor} ^ \left|{i - j}\right|
\end{align}
        
\noindent where the correlation factor takes on a value between $0$ and $1$ \citep{zou2005regularization}.

\subsection{The effects of high dimensionality on a multi-variate OLS regression}

\noindent Our first simulation exercise seeks to observe the reliability of our OLS beta estimates for data sets with different degrees of dimensionality. We fix our number of observations to $n = 30$, set all $\beta$'s to $5$, and vary the number of parameters such that $p = \{2, 15, 20, 28\}$. We then simulate $1000$ randomly drawn data sets for each case of dimensionality and perform multi-variate OLS regressions to derive the estimated beta coefficients and their corresponding variances. In the case of low dimensionality, where $p = 2$, we observe a low estimated variance for our $\hat{\beta}$'s. However, as $p$ increases, the variance of our estimated beta coefficients increases on average. \\

\noindent In conjunction with our anticipated results for the variance of the beta estimates, the coefficient estimates themselves remain unbiased on average and correspond to the true beta coefficients. Although the OLS coefficient estimates remain unbiased, this increase in variability among the OLS coefficient estimates introduces uncertainty into the results of the fitted model such that the derived estimates for any given sample are not guaranteed to represent the true beta values. In Figure \ref{fig:old_distr}, the plotted distributions of the beta coefficients confirm these expected outcomes of unbiasedness and increased variability when performing traditional OLS regressions on high-dimensional data sets.

\subsection{Regularization methods and their shrinkage characteristics}

\noindent In this section, we first introduce a series of basic Monte-Carlo simulations to demonstrate the continuous shrinkage property of the ridge regression on average, which is summarized in Figure \ref{fig:avg_ridge_betas}.\footnote{The rgressors where standarized before running any regularization regression to assure all features have the same scale.} Maintaining a similar simulation setup as above in our OLS exercise, we draw $500$ random data samples from our constructed DGP with $n=30$ for each degree of dimensionality represented by $p = \{2, 28, 30, 35\}$ and perform ridge regressions with incrementally larger values of the tuning parameter $\lambda$. We then calculate the averages for each beta and the corresponding average variance as $\lambda$ increases. When $\lambda = 0$, then $\hat{\beta}^R = \hat{\beta}_{OLS}$. Note that for a case where $p=35$, the OLS estimate does not exist since the matrix $\mathbf{X}$ does not have full rank. \\

\noindent As $\lambda$ increases, the ridge constraint becomes more restrictive and further shrinks the beta coefficients toward zero on average. For data samples with low dimensionality, it is apparent that there is little need to employ a ridge regression: For $\lambda$ values up to $1$, the coefficients remain stable around the true beta values and only begin to shrink once the tuning parameter becomes excessively large. As the number of $p$ parameters increases, however, we observe how the beta coefficients further deviate from their true values in the least squares case due to variance inflation and become increasingly sensitive to marginal changes in $\lambda$. In the extreme case where $p = 35$, for example, the ridge coefficients shrink rapidly and already begin stabilizing when $\lambda$ is around $10e-12$. As the average coefficient estimates shrink continuously for increasing values of $\lambda$, we also observe a substantial reduction in the average variance of the $\hat{\beta}$'s, which improves the efficiency of the beta estimates generated for any given data sample. Due to the monotone shrinkage of the beta coefficients for the simulated data sets, we further see a simultaneous rise in bias, characterized by the increasing deviation between the average size of the estimated beta coefficients and the true beta. These results are visually summarized in Figure \ref{fig:ridge_shrunken_betas}: The distributions of the coefficients for a low $\lambda$ value in the left-hand plot exhibit minimal bias with inflated variance. As $\lambda$ increases, however, the mean of the distributions steadily begin to deviate from the true mean, as we observe a simultaneous reduction in the variance of $\hat{\beta}^R$. \\  

\noindent In Section \ref{section:lasso}, we showed that the lasso prediction error bound will be smaller as the sparsity index is low (i.e., the number of zero coefficients is large). The graphs in Figure \ref{fig:avg_lasso_betas} clearly exhibit this characteristic of lasso. Drawn from $100$ i.i.d. random samples, each Monte-Carlo simulation depicted in the four quadrants represents a high-dimensional setting with $30$ observations and $35$ regressors with increasing degrees of sparsity. Starting with the upper two quadrants, we demonstrate ideal scenarios of high sparsity with $s_0=5$ and $s_0=15$, respectively. The lasso regression successfully estimates the non-zero coefficients to be near the true beta coefficient of $5$ and sets all remaining coefficients to zero. As the degree of sparsity decreases, the lasso regressions depicted in the two lower quadrants exhibit a wider range of estimates for the non-zero beta coefficients and struggle to shrink the estimates of the truly zero coefficients to zero. \\

\noindent In a second simulation, we show in Figure \ref{fig:lasso_shrunken_betas} that as the $\lambda$ tuning parameter increases, lasso estimates for non-zero regressors will achieve a reduction in their variance as shown by the red $\hat{\beta}^L$ distribution. The non-zero coefficients remain relatively unbiased on average and do not shrink to zero given that their correlation with the residual, $c_j$, is sufficiently large. For extremely large values of $\lambda$, however, lasso will set even relevant coefficients to zero and the green distribution that represents the average of zero coefficients will disappear, as its variance approaches zero. 
\\

\noindent While the Monte-Carlo simulations for ridge and lasso indicate that their beta coefficients and respective variance shrink monotonously on average, note that this monotone shrinkage behavior may not be the case as we plot the ridge and lasso beta estimates against different values of the tuning parameter for just one random sample. In figures \ref{fig:ridge_single_sample} and \ref{fig:lasso_single_sample}, we provide a single randomly selected data set for various degrees of dimensionality and trace the size of the beta coefficients as a function of $\lambda$. On many occasions it is not uncommon for a positive (negative) ridge or lasso coefficient estimate to initially increase (decrease) before shrinking toward zero. Alternatively, a coefficient estimate may start out with the correct sign and then change to the wrong sign for some value or range of $\lambda$ before correcting itself for larger values of the tuning parameter.\\ 

\noindent To showcase the characteristics of the naive elastic net, we conduct a series of simulations for which we plot the average elastic net beta estimates as a function of $\lambda$ for different magnitudes of the $\ell_1$ ratio. We follow a similar setup as in previous simulations of high dimensionality by assigning the number of observations to $30$ and the number of regressors to $35$. We highlight an optimal scenario in which the naive elastic net will outperform lasso by introducing a moderate level of sparsity with $10$ regressors set to $\beta=2$ and with all remaining regressors set to $\beta=0$. We furthermore add a high magnitude of pairwise correlation among all regressors, setting the correlation factor in equation \eqref{eqn:corrmatrix} to $0.7$. Due to limited computing power the number of random draws for our simulation is $500$. Starting in the upper left corner of Figure \ref{fig:elnet_avg_betas}, we plot our results for the case where the $\ell_1$ ratio is equal to $0$. As this scenario places all weight on the $\ell_2$-norm, it is equivalent to the ridge regression. Due to the moderate degree of sparsity, ridge struggles to estimate the regressors with a true beta coefficient of zero. As the $\ell_1$ ratio increases, the naive elastic net is able to mitigate the drawbacks of the ridge regression, improving how effectively sparse regressors shrink to zero. With an $\ell_1$ ratio of $1$, the final plot replicates a lasso regression. \\

\noindent Given the degree of sparsity and pairwise correlation among regressors, the elastic net serves as an appropriate mediator that balances the advantages associated with both the $\ell_1$- and $\ell_2$-norms. However, our resulting simulations summarized in Figure \ref{fig:elnet_avg_betas} indicate that the lasso regression (where the $\ell_1$ ratio has been set to 1) outperforms all cases of the naive elastic net for which $\ell_1$ ratio $\in (0,1)$. As discussed above in Section \ref{section:theory}, the naive elastic net only performs well in settings where it is very close to either ridge or lasso. Based on the results of our simulation, the naive elastic net therefore does not outperform the lasso regression. Although the circumstances of our simulation exercise are not ideal for highlighting how the robust elastic net can outperform lasso in a situation with moderate sparsity and high pairwise correlation, Figure \ref{fig:elnet_avg_betas} still successfully shows the general mechanics of how the (naive) elastic net serves as an intermediate model between ridge and lasso that combines the continuous shrinkage property of ridge and the variable selection property of lasso. Figure \ref{fig:elnet_shrunken_betas} shows the bias-variance trade-off for the elastic net scenario. For simplicity we report the graph holding fix the $\ell_1$-ratio to $0.5$. Clearly, as $\lambda$ increases the peak of the orange bell curve for the true non-zero coefficients moves away from the dashed vertical line, which indicates the true value of the beta coefficients. Although the bias increases, a major gain is obtained in terms of decreasing variance, as we can see in the x-axis the interval enclosing the tails becomes smaller.

\subsection{Test mean squared error simulations}
\label{section:msesim}

\noindent We now compare their prediction performance by using the test MSE as a measure for goodness-of-fit to analyze a number of scenarios in which we vary some of the most important characteristics of the methods: high dimensionality, sparsity index, and degree of multicollinearity. All simulations for this part of our analysis are limited to $500$ random samplings due to limited computing power. The results from each simulation are summarized below in Section \ref{ap:tables}. \\ 

\noindent We begin by examining case \ref{tab:case1} with high dimensionality and varying degrees of sparsity, where we have $30$ observations and $35$ explanatory variables that are absent of any pairwise correlation among regressors. All truly non-zero beta coefficients are set to $\beta=2$. In the case of high sparsity, where there are a total of $10$ non-zero betas, the test MSE is smallest for the lasso regression. As we achieve a moderate level of sparsity by increasing the number of non-zero betas to $20$, the naive elastic net regression with an $\ell_1$ ratio of $0.2$ yields the lowest test MSE. When we replicate a scenario in which there is no sparsity at all, we observe that the ridge model outperforms all others. These results fully correspond with our expectations from the theory section: as the sparsity index $s_0$ gets smaller, the bounds of the prediction error for lasso become narrower and thus lasso will exhibit good prediction performance. Also, lasso performs better when there is low or no correlation among regressors. On the other hand, ridge is optimal under circumstances of low sparsity in which very few explanatory variables have a true beta coefficient of zero. For cases of moderate sparsity, the elastic net serves as a hybrid model that adopts benefits of continuous shrinkage from ridge and variable selection from lasso to avoid model overfitting. \\  

\noindent Case \ref{tab:case2} simulates low-dimensional data sets with $30$ observations and only $10$ regressors, moderate to high pairwise correlation with the correlation factor set to $0.8$, and varying degrees of sparsity. As in the previous case, all truly non-zero betas are set to $\beta=2$. For the first two simulations that introduce high and moderate sparsity with a total of three and seven non-zero betas, respectively, the naive elastic net with an $\ell_1$ ratio of $0.7$ yields the lowest test MSE. In the case where all ten regressors have truly non-zero betas, the ridge model performs comparatively better than the naive elastic net and lasso. Again, these results are consistent with the theory of regularization methods. Although lasso perform well in cases of high sparsity, it struggles with pairwise correlation between regressors, as it will select just one regressor out of the subset of correlated regressors and set the other ones to zero. This could imply leaving out relevant features from the model. Our results suggest that a (naive) elastic net model could improve overall model fit by incorporating the continuous shrinkage characteristic of ridge regression. As such, it is unsurprising that the naive elastic net model with an $\ell_1$ ratio of $0.7$ improves prediction performance in this scenario. Likewise, a (naive) elastic net model is most suitable in the presence of moderate sparsity (with or without the presence of pairwise correlation). We also anticipate that ridge should outperform all other models when little to no sparsity is present.          \\

\noindent In case \ref{tab:case3}, we follow the simulation exercise in \cite{zou2005regularization} for simulating samples that contain low dimensionality with high sparsity and varying degrees of pairwise correlation. The simulated data sets include $20$ observations and 8 regressors, with $\beta=\{3, 1.5, 0, 0, 2, 0, 0, 0\}$. No matter the degree of pairwise correlation introduced into the data sets, the lasso model sufficiently outperforms all other models. From a theoretical perspective, we anticipate that an elastic net model should generate the lowest test MSEs given the degree of introduced sparsity and pairwise correlation. Indeed, this is the outcome reported by \cite{zou2005regularization}. As we were limited to using only the naive elastic net from the scikit-learn package, we were unable to fully replicate the simulation exercise outlined by the authors. As discussed above, the naive elastic net performs best under specific conditions where it is very close to ridge or lasso. As such, our results do not contradict the theory of regularization methods. Rather, they highlight an important distinction between the naive elastic net and its more robust counterpart. \\

\noindent Case \ref{tab:case4} closely follows the setup outlined in the third case with the introduction of varying degrees of pairwise correlation and is also derived from \cite{zou2005regularization}. However, all beta coefficients are assigned a value of $0.85$. As is to be expected, the ridge regression performs optimally in all simulations due to the absence of sparsity in the simulated data sets.      \\

\noindent Finally, case \ref{tab:case5} replicates a data set with high dimensionality and high sparsity, where the number of observations is $30$ and the number of regressors is $35$. In all simulations of case five, we have assigned $10$ non-zero betas a value of $2$ with the remaining regressors having a truly zero beta coefficient. We then vary the degree of pairwise correlation among regressors. For the simulations with low (correlation factor of $0.1$) and moderate (correlation factor of $0.3$) pairwise correlation, the lasso model yields the lowest test MSE. However, once the pairwise correlation among regressors achieves a substantially higher value with a correlation factor of $0.7$, the naive elastic net that has an $\ell_1$ ratio of $0.7$ performs best. Since the first two simulation exercises in case 5 exhibit low and moderate pairwise correlation plus high sparsity, respectively, we accurately anticipate from the theory outlined above that lasso should perform optimally. As the pairwise correlation increases to a correlation factor of $0.7$, however, lasso struggles to mitigate the confounding effects of high collinearity and is therefore outperformed by the naive elastic net model with an $\ell_1$ ratio of $0.7$. Due to the combination of high dimensionality and substantial pairwise correlation, the naive elastic net model suitably counterbalances the drawbacks of the lasso regression by incorporating the continuous shrinkage features of ridge.  \\

\noindent As a final simulation exercise, we explore the use of LOOCV and 10-fold cross-validation in order to select the optimal value for the $\lambda$ tuning parameter. The results of this simulation can be found in Section \ref{section:cvsim} of the Appendix. 




