\section{Conclusion}
\label{section:conclusion}

\noindent As a baseline, we first demonstrate the weaknesses of the least squares model in the presence of high-dimensionality and multicollinearity. We then  present three regularization models - ridge, lasso, and (naive) elastic net - commonly used to mitigate these drawbacks by analytically and graphically illustrating their properties. Specifically, ridge regression has a comparative advantage with respect to other regularization models when there is low sparsity and high pairwise correlation. On the contrary, lasso achieves better results in the case of high sparsity and low pairwise correlation. Finally, these two methods can be combined, resulting in the (naive) elastic net which, in turn, can further improve prediction performance by combining the best features of ridge and lasso. %Despite its appealing upgrade, its intrinsic naive characteristic limits elastic net optimal performance to the proximity of either ridge or lasso.
\\

\noindent We proceed with several Monte-Carlo simulations expecting to return the basic characteristics suggested by the theoretical background outlined in Section \ref{section:theory}. Our analyses emphasize the average shrinkage effects of each model by observing the behavior of the coefficient paths as a function of the $\lambda$ tuning parameter and the distributions of the coefficient estimates for specific values of $\lambda$. We further present a series of simulations that compare the prediction performance of all three regularization methods using the test MSE. In each simulation exercise, we vary the degree of dimensionality, sparsity, and multicollinearity among regressors. After linking the theory with the computed simulations, we conclude by applying the regularization models to a real data set from \citet{stamey1989prostate}. Due to limitations of the scikit-learn package, we were unable to provide analyses using the more robust version of the elastic net model that employs the LARS-EN algorithm.    \\

\noindent The results from our data application reflect the observations made in our theoretical discussion of regularized regression and in our simulation exercises. The conducted analyses help to grasp the functioning mechanisms of the three regularization models and ultimately show that there is no "one size fits all" method that performs optimally under all circumstances, as the selection of a suitable regularization model depends heavily on the characteristics of a given data set.\\


