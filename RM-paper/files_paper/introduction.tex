\section{Introduction}
The traditional least squares estimator might not be the most desirable method for prediction purposes under certain conditions. Take as an example a data set that contains highly correlated regressors. Although the least squares estimators remain unbiased, their variance increases due to the presence of multicollinearity, which compromises prediction accuracy of the outcome variable. Moreover, when the number of regressors is large, least squares might suffer from interpretation issues, as it is difficult to pin down the subset of regressors that have the strongest effects. When the number of predictors, $p$, is larger than the number of observations, $n$, in cases of high-dimensionality, it no longer even possible to compute the least squares regression estimators. \\

\noindent Regularization methods can help to overcome these problems by shrinking the coefficients towards zero. In so doing, these methods sacrifice a small amount of bias to significantly reduce the variance of the estimators in order to gain prediction accuracy. As we will see, particular regularization methods even perform variable selection by setting some of the estimated coefficients to zero. This feature becomes increasingly relevant when the number of predictors in a model high, and it is unknown which ones are relevant for accurately predicting the response variable.\\

\noindent The first regularization method we discuss below is ridge regression, which was first introduced by \cite{hoerl1970ridge1} as an alternative model for OLS when prediction vectors are non-orthogonal (i.e., the correlation between predictors is not zero). The authors show that by taking the traditional OLS estimate closed form solution and augmenting the diagonal of $\mathbf{X}^{\prime} \mathbf{X}$ matrix by a small and positive parameter, one can reduce the variance of the estimator at the cost of a marginal increase in its bias. %Even though the ridge estimator is biased, it is associated with a lower mean squared error than the OLS estimator, thus improving prediction performance. %
%\cite{frank1993statistical} later discuss ridge regression as a penalty in the minimization problem of residuals sum of square. 
\\


\noindent \cite{tibshirani1996regression} point out some drawbacks of the ridge model outlined by \cite{hoerl1970ridge1}. In particular, ridge regression shrinks coefficients continuously towards zero. However, it is incapable of setting any parameter estimate exactly to zero. The author therefore proposes a new method called \textit{least absolute shrinkage and
selection operator}, or simply, \textit{lasso}. %By applying a differnt constraint to the The lasso applies another type of constraint to the residual sum of squares and thus can produce estimates that are exactly zero. %This property is mostly desirable in a scenario of high-dimensionality, since it makes it possible to compute estimates when the number of features is larger than the number of observations%
A key assumption of the method is the sparsity condition, which means that lasso assumes that the true process is characterized by having a sufficient number of coefficients that are not relevant for predicting the outcome variable.\\
% As the lasso solution is a quadratic programming problem, there exist several algorithms that compute the entire lasso path, such as LARS (\cite{efron2004least}) and coordinate descent. \\


\noindent The literature related to variants of the lasso is extensive, but here we mention a few: grouped lasso for grouped variable selection, for example, in settings with categorical inputs (\cite{yuan2006model}); fused lasso for sparse functional data analysis (\cite{chen2010graph}); adaptive lasso, which assigns adaptive weights for different coefficients (\cite{zou2006adaptive}); and graphical lasso, an algorithm to estimate sparse graphs (\cite{friedman2008sparse}). See \cite{tibshirani2011regression} for an extended list of generalizations of the lasso. \\

\noindent Despite the desirable properties of the lasso model, \cite{zou2005regularization} point out some drawbacks of the method including its difficulties handling highly correlated variables (where ridge will actually outperform lasso) and its saturation limit, which prevents lasso from  selecting all relevant regressors when the number of predictors is larger than the number of observations. They therefore introduce elastic net regression, a hybrid approach between lasso and ridge regression to mitigate these problems. \\


% \noindent Despite the apparent strengths of Hoerl and Kennard's ridge regression, the model does not serve as a general solution for all cases in which the least squares estimates exhibit high variance. In his seminal paper, Tibshirani proposes an alternative regularization regression method called the "least absolute shrinkage and selection operator," or simply \textit{lasso}. Unlike ridge, which shrinks the coefficient estimates, but never sets any of the $\hat{\beta}^{R}$'s to exactly zero, Tibshirani's lasso model reduces the magnitude of larger coefficients and sets smaller ones to zero. As such, lasso attempts to reduce the inflated variability of a least squares estimation in the presence of high dimensionality, while improving model interpretability through variable selection. \\ 

\noindent Our study focuses on the prediction accuracy of the three most common regularization methods: ridge, "vanilla" lasso, and the (naive) elastic net. Our paper is structured as follows: Section \ref{section:theory} discusses the statistical properties of each shrinkage regression and provides a theoretical foundation of their characteristics. Section \ref{section:mse} discusses model assessment and selection of the optimal tuning parameter, $\lambda$. Section \ref{section:simulation} presents the main results of our Monte-Carlo simulations to demonstrate the drawbacks of the OLS estimator under multicollinearity and high dimensionality. We also present our results for the prediction performance of all three regularization methods under different cases of multicollinearity, dimensionality, and degrees of sparsity. We further evaluate their prediction accuracy by comparing the models' mean squared errors (MSE). Section \ref{section:data} presents an application to a real data set,
and Section \ref{section:conclusion} concludes.  All proofs, extensions, figures, and tables can be found in Section \ref{section:appendix}. 
 
